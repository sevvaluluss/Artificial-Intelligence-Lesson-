{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sevvaluluss/Artificial-Intelligence-Lesson-/blob/main/Lab_03_Sevval_Ulus_Memis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1a0c6b-8a0b-4383-8b5c-104e7df527c3",
      "metadata": {
        "tags": [],
        "id": "5e1a0c6b-8a0b-4383-8b5c-104e7df527c3"
      },
      "source": [
        "# COSC-x40: Lab 3\n",
        "\n",
        "**Sevval Ulus Memis**\n",
        "\n",
        "[PLACEHOLDER_FOR_NOTEBOOK_LINK]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "Follow the instructions below to copy this notebook and to perform some initial setup.\n",
        "\n",
        "1. Copy this notebook by selecting `File > Save a copy in Drive`.\n",
        "2. A new window should open for the copied notebook. Move the new notebook to your course folder in Google Drive by selecting `File > Move` and then selecting the desired folder.\n",
        "3. Update the name of the notebook by removing \"Copy of\" and replacing \"Username\" with your actual username.\n",
        "4. Update the first cell in the notebook by specifying your course number (440 or 640) and by replacing \"**Student Name**\" with your actual name.\n",
        "5. Do not edit the line that says `PLACEHOLDER_FOR_NOTEBOOK_LINK`. This will be used by the [Notebook Renderer](https://colab.research.google.com/drive/1CJTipys46ldZxJFwnt7XbdjQUfkmoXeU?usp=sharing) tool to insert a link to your Colab notebook.\n",
        "6. Enable link sharing for your notebook."
      ],
      "metadata": {
        "id": "qpayF6_U5eWL"
      },
      "id": "qpayF6_U5eWL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Colab Environment\n",
        "\n",
        "Run the cell below to download the `aitools` course package."
      ],
      "metadata": {
        "id": "EIH5BcjzD2sl"
      },
      "id": "EIH5BcjzD2sl"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm aitools -r\n",
        "!git clone https://github.com/drbeane/aitools.git"
      ],
      "metadata": {
        "id": "dK4wTPOiG6lA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dK4wTPOiG6lA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to import the necessary tools for this assignment. **No other import statements are required for this Lab, and no other import statements should be included in this assignment.**"
      ],
      "metadata": {
        "id": "25OvYckaD7g7"
      },
      "id": "25OvYckaD7g7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hULDHBL5h_5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from aitools.algs import play_game, tournament\n",
        "from aitools.algs import RandomPlayer, GreedyPlayer, MinimaxPlayer\n",
        "from aitools.envs import Oware"
      ],
      "id": "-hULDHBL5h_5"
    },
    {
      "cell_type": "markdown",
      "id": "edef32b8-886f-4e07-9713-e39eafe8f24a",
      "metadata": {
        "tags": [],
        "id": "edef32b8-886f-4e07-9713-e39eafe8f24a"
      },
      "source": [
        "# Part 1: Playing Individual Games\n",
        "\n",
        "In Part 1, you will then run a few sample games of Oware with different agents playing against each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ek3GACimYw5r"
      },
      "id": "ek3GACimYw5r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.A - Checking Heuristic Values\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 1'`.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 2'`.\n",
        "\n",
        "Use the `play_game()` function to have the agents play 50 turns of a game of Oware, setting `random_state=1`. Leave `display_flags` with its default value. Use the `display()` method of the `Oware` class to display the game state after 50 moves. **Note: You should display the last game state, not the initial state.**\n",
        "\n",
        "Then print the heuristic value of the current game state from the perspective of each player, along with messages like this shown below:\n",
        "\n",
        "    Estimated board value for Player 1 is: ____\n",
        "    Estimated board value for Player 2 is: ____"
      ],
      "metadata": {
        "id": "rU7fR-9o9T_G"
      },
      "id": "rU7fR-9o9T_G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3deed7-8378-4da7-9077-2fd68e4fa2cc",
      "metadata": {
        "tags": [],
        "id": "4f3deed7-8378-4da7-9077-2fd68e4fa2cc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.B - Random vs Random\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 1'`.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 2'`.\n",
        "\n",
        "Use the `play_game()` function to have the agents play a complete game of Oware. Set `random_state=1` and `display_flags='wt'` when calling `play_game()`. Use the `display()` method of the `Oware` class to display the final game state. **Note: You should display the final game state, not the initial state.**\n"
      ],
      "metadata": {
        "id": "7UUoR-rK-wQr"
      },
      "id": "7UUoR-rK-wQr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4662bdee-8f7f-4cdf-a612-7d2aaa7ead35",
      "metadata": {
        "tags": [],
        "id": "4662bdee-8f7f-4cdf-a612-7d2aaa7ead35"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.C - Random vs Greedy\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player'`.\n",
        "* An instance of the `GreedyPlayer agent` named `'Greedy Player'`.\n",
        "\n",
        "Use the `play_game()` function to have the agents play a complete game of Oware with the `RandomPlayer` agent playing first. Set `random_state=1` and `display_flags='wt'` when calling `play_game()`. Use the `display()` method of the `Oware` class to display the final game state. **Note: You should display the final game state, not the initial state.**"
      ],
      "metadata": {
        "id": "5Rtfp8Vg_C1e"
      },
      "id": "5Rtfp8Vg_C1e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e382c6b6-45c3-4510-8200-d74b92013c69",
      "metadata": {
        "tags": [],
        "id": "e382c6b6-45c3-4510-8200-d74b92013c69"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.D - Greedy vs Minimax\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `GreedyPlayer` agent named `'Greedy Player'`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` named `'Minimax Player'`.\n",
        "\n",
        "Use the `play_game()` function to have the agents play a complete game of Oware with the `GreedyPlayer` agent playing first. Set `random_state=1` and `display_flags='wt'` when calling `play_game()`. Use the `display()` method of the `Oware` class to display the final game state. **Note: You should display the final game state, not the initial state.**"
      ],
      "metadata": {
        "id": "-EBUzx6R_st9"
      },
      "id": "-EBUzx6R_st9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5be2e93-d37e-4aff-b0e1-fbd8356e04a0",
      "metadata": {
        "tags": [],
        "id": "d5be2e93-d37e-4aff-b0e1-fbd8356e04a0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6e012e67-0253-4b8a-a014-4ac597a7e8a4",
      "metadata": {
        "id": "6e012e67-0253-4b8a-a014-4ac597a7e8a4"
      },
      "source": [
        "# Part 2: Running Tournaments\n",
        "\n",
        "In Part 2, you will compare the performance of different agents by having them compete against each other in tournaments."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.A - Random vs Random\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 1'`.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 2'`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and alternate the starting player between games.\n"
      ],
      "metadata": {
        "id": "yxdvLZUkAZR2"
      },
      "id": "yxdvLZUkAZR2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5d2559-d2e7-4487-a375-55457af76837",
      "metadata": {
        "tags": [],
        "id": "1d5d2559-d2e7-4487-a375-55457af76837"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.B - Random vs Greedy\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player'`.\n",
        "* An instance of the `GreedyPlayer` agent named `'Greedy Player`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and alternate the starting player between games.\n",
        "\n",
        "**Note:** When specifying the agents for the tournament, please list the `RandomPlayer` agent first.\n"
      ],
      "metadata": {
        "id": "hUfUriLyBIDr"
      },
      "id": "hUfUriLyBIDr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d6660a-31f7-4e57-bead-508190173d40",
      "metadata": {
        "tags": [],
        "id": "25d6660a-31f7-4e57-bead-508190173d40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "edecc9b5-c285-4914-8b5c-5389f604eb2b",
      "metadata": {
        "id": "edecc9b5-c285-4914-8b5c-5389f604eb2b"
      },
      "source": [
        "## 2.C - Random vs Minimax\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player'`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` named `'Minimax Player`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and alternate the starting player between games.\n",
        "\n",
        "**Note:** When specifying the agents for the tournament, please list the `RandomPlayer` agent first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bec05d-38b5-4671-b6de-60545eb95a0f",
      "metadata": {
        "tags": [],
        "id": "56bec05d-38b5-4671-b6de-60545eb95a0f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.D - Greedy vs Minimax\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `GreedyPlayer` agent named `'Greedy Player'`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` named `'Minimax Player`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and alternate the starting player between games.\n",
        "\n",
        "**Note:** When specifying the agents for the tournament, please list the `GreedyPlayer` agent first.\n"
      ],
      "metadata": {
        "id": "wDQyE1a3ClD1"
      },
      "id": "wDQyE1a3ClD1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977c286c-f605-46c5-8702-16e26688619b",
      "metadata": {
        "tags": [],
        "id": "977c286c-f605-46c5-8702-16e26688619b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.E - Minimax(2) vs Minimax(3)\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=2` named `'Minimax(2) Player`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` named `'Minimax(3) Player`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and alternate the starting player between games.\n",
        "\n",
        "**Note:** When specifying the agents for the tournament, please list the `MinimaxPlayer` agent with `depth=2` first."
      ],
      "metadata": {
        "id": "G5M0ivl4Cte2"
      },
      "id": "G5M0ivl4Cte2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbebe50-9a86-456c-97b7-eaa213c161d7",
      "metadata": {
        "id": "bcbebe50-9a86-456c-97b7-eaa213c161d7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "efe573fc-aca6-4dbe-886d-7a971b79992d",
      "metadata": {
        "id": "efe573fc-aca6-4dbe-886d-7a971b79992d"
      },
      "source": [
        "# Part 3: Exploring Play Order Advantage\n",
        "\n",
        "Oware is a deterministic game in which both players have complete information about the game state at all times. For any turn-based game with these characteristics, one of the following conditions will be true:\n",
        "\n",
        "1. The first player will always win if playing optimally.\n",
        "2. The second player will always win if playing optimally.\n",
        "3. The game will be a draw if both players are playing optimally.\n",
        "\n",
        "For example, [tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe) always ends in a draw if both players are playing optimally, but the first player will always win [Connect Four](https://en.wikipedia.org/wiki/Connect_Four) if playing optimally. You can read more about this concept here: [Wikipedia: First-player and second-player win](https://en.wikipedia.org/wiki/First-player_and_second-player_win)\n",
        "\n",
        "It is known that the first player will always win in Oware if playing optimally. But our agents should not be expected to play optimally. In Part 3, you will investigate if any of the agents appear to have an advantage when playing either first or second. We will explore this by running tournaments in which a specific agent will play against itself without changing the player order between games."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.A - Random Agent and Turn Order Advantage\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 1'`.\n",
        "* An instance of the `RandomPlayer` agent named `'Random Player 2'`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and **DO NOT** alternate the starting player between games.\n"
      ],
      "metadata": {
        "id": "Hn1e3DbbDDVo"
      },
      "id": "Hn1e3DbbDDVo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c3fff8-bdde-42c3-b167-62e1007735e9",
      "metadata": {
        "id": "37c3fff8-bdde-42c3-b167-62e1007735e9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.B - Greedy Agent and Turn Order Advantage\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `GreedyPlayer` agent named `'Greedy Player 1'`.\n",
        "* An instance of the `GreedyPlayer` agent named `'Greedy Player 2'`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and **DO NOT** alternate the starting player between games."
      ],
      "metadata": {
        "id": "SvIjVCfsD2FW"
      },
      "id": "SvIjVCfsD2FW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c16945-d10c-4612-b932-168aa1a87542",
      "metadata": {
        "id": "69c16945-d10c-4612-b932-168aa1a87542"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.C - Minimax(2) Agent and Turn Order Advantage\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=2` named `'Minimax Player 1'`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=2` named `'Minimax Player 2'`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and **DO NOT** alternate the starting player between games."
      ],
      "metadata": {
        "id": "vb0fRIFhELXV"
      },
      "id": "vb0fRIFhELXV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb495d67-f84a-4904-b613-2bdb7f3b50d7",
      "metadata": {
        "id": "bb495d67-f84a-4904-b613-2bdb7f3b50d7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.D - Minimax(3) Agent and Turn Order Advantage\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` named `'Minimax Player 1'`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` named `'Minimax Player 2'`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 500 games of Oware. Set `random_state=1` and **DO NOT** alternate the starting player between games."
      ],
      "metadata": {
        "id": "IJpcVK15Ed7Q"
      },
      "id": "IJpcVK15Ed7Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5eaf44-940d-4b6f-b1c4-e03fe68aee1e",
      "metadata": {
        "id": "8c5eaf44-940d-4b6f-b1c4-e03fe68aee1e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "efa7c263-94f8-4a74-aa8a-6499ca4591c3",
      "metadata": {
        "id": "efa7c263-94f8-4a74-aa8a-6499ca4591c3"
      },
      "source": [
        "# Part 4: Effects of Alpha-Beta Pruning\n",
        "\n",
        "In Part 4, you will compare the results and runtimes for MiniMax players with and without alpha-beta pruning.\n",
        "\n",
        "We will start by verifying that the MiniMax agent produces the same results whether or not alpha-beta pruning is used. This is done in Parts 4.A and 4.B."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.A - Minimax (2) vs Minimax (3), without ABP\n",
        "\n",
        "Create the following objects:\n",
        "* An instance of the `Oware` environment.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=2` and `ABP=False`. Name the agent `'Minimax(2) Player'`.\n",
        "* An instance of the `MinimaxPlayer` agent with `depth=3` and `ABP=False`. Name the agent `'Minimax(3) Player'`.\n",
        "\n",
        "Use the `tournament()` function to have the agents play 200 games of Oware. Set `random_state=1` and alternate the starting player between games.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HwHE_cX4Ew-4"
      },
      "id": "HwHE_cX4Ew-4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473bf3db-f97d-432b-bba6-1fa8908aa9a1",
      "metadata": {
        "id": "473bf3db-f97d-432b-bba6-1fa8908aa9a1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.B - Minimax (2) vs Minimax (3), with ABP\n",
        "\n",
        "Repeat the instructions from 4.A, but this time set `ABP=True` for the two agents. You should see that the results of the tournament are exactly the same regardless of whether or not ABP is used. The search times for the algorithms should be somewhat different, however."
      ],
      "metadata": {
        "id": "jlvqeXXdFnOy"
      },
      "id": "jlvqeXXdFnOy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafa4c71-8746-4bf5-b06d-edac48186f0a",
      "metadata": {
        "id": "cafa4c71-8746-4bf5-b06d-edac48186f0a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2ca0a118-204d-49a0-99eb-bd8d3bbeb619",
      "metadata": {
        "id": "2ca0a118-204d-49a0-99eb-bd8d3bbeb619"
      },
      "source": [
        "## 4.C - Search Time with and Without ABP\n",
        "\n",
        "We will now more thoroughly explore the impact that alpha-beta pruning has on search time. We will run several 10-game tournaments between a Random Player and a Minimax player. For each Minimax depth from 2 to 6, we will run two tournament: One with ABP and one without ABP. For each tournament, we will store the total search time for the Minimax player into one of two lists, which will be named `without_ABP` and `with_ABP`.\n",
        "\n",
        "Code has been provided for you for this task. Run the cell below as-is. This cell will likely take around 3-4 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26fb701c-eaf9-4905-bd60-ec91db69fb7c",
      "metadata": {
        "id": "26fb701c-eaf9-4905-bd60-ec91db69fb7c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "without_ABP = []\n",
        "with_ABP = []\n",
        "\n",
        "game = Oware()\n",
        "for d in [2,3,4,5,6]:\n",
        "    print(f'Running tournaments with d={d}')\n",
        "    p1 = RandomPlayer('Random')\n",
        "    p2 = MinimaxPlayer('Minimax', depth=d, ABP=False)\n",
        "    p3 = MinimaxPlayer('Minimax', depth=d, ABP=True)\n",
        "\n",
        "    results = tournament(\n",
        "        game, [p1, p2], rounds=10, display_results=False, return_results=True\n",
        "    )\n",
        "    without_ABP.append(results['play_time'][2])\n",
        "\n",
        "    results = tournament(\n",
        "        game, [p1, p3], rounds=10, display_results=False, return_results=True\n",
        "    )\n",
        "    with_ABP.append(results['play_time'][2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.D - Plotting Results\n",
        "\n",
        "Use Matplotlib to create a figure illustrating the runtimes for each Minimax agent in Task D. The figure should adhere to the following specifications:\n",
        "\n",
        "* It should contain two linesplots on the same set of axes. One lineplot will represent values in `without_ABP` and the other will represent values in `with_ABP`.\n",
        "* The horizontal axis should indicate the agent depth (2 - 6).\n",
        "* The figure should contain a legend to indicate which curve is associated with which agent. Use `Without ABP` and `With ABP` for the labels.\n",
        "* The horizontal and vertical axes should be labeled as \"Minimax Depth\" and  \"Runtime (in seconds)\".\n",
        "* The plot should display gridlines (use `plt.grid()` for this)."
      ],
      "metadata": {
        "id": "RjZxytl5IA68"
      },
      "id": "RjZxytl5IA68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa49f3f7-9e23-4732-bb11-706f06914a13",
      "metadata": {
        "id": "fa49f3f7-9e23-4732-bb11-706f06914a13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Instructions\n",
        "\n",
        "1. Perform a Restart and Run All by clicking **Runtime > Restart session and run all**.\n",
        "2. Copy the link to your notebook by clicking **Share > Copy Link**.\n",
        "3. Paste the copied link into the `notebook_url` field in the [Notebook Renderer](https://colab.research.google.com/drive/1CJTipys46ldZxJFwnt7XbdjQUfkmoXeU?usp=sharing) tool and then execute the cell to render the notebook.\n",
        "4. The Notebook Renderer will open up a save file dialog. Save the resulting HTML file yo your local machine.\n",
        "5. Submit the HTML file to Canvas.\n"
      ],
      "metadata": {
        "id": "SKt9nozPI5gI"
      },
      "id": "SKt9nozPI5gI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}